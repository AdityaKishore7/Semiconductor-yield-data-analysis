{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 36825,
          "sourceType": "datasetVersion",
          "datasetId": 28901
        }
      ],
      "dockerImageVersionId": 28449,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "FMST Semiconductor manufacturing project",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TLcZB0CZjHMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Some Basic Libraries"
      ],
      "metadata": {
        "id": "8oSKDLyvjHMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for basic operations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# for visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "# for modeling\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "# to avoid warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# for providing path\n",
        "import os\n",
        "print(os.listdir(\"../input\"))\n"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "id": "nJL_lOYIjHMp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading the Data and understanding the attributes"
      ],
      "metadata": {
        "id": "Lbdc02UhjHMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the data\n",
        "data = pd.read_csv('../input/uci-secom.csv')\n",
        "\n",
        "# getting the shape of the data\n",
        "# we have 1,567 rows and 592 columns\n",
        "print(data.shape)\n"
      ],
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "kOFWzuTpjHMr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the head of the data\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "_uuid": "a6ef0c18b654cdf34cbae029d40977007d86c2f1",
        "trusted": true,
        "id": "7IxN8syAjHMs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# checking if the dataset contains any NULL values\n",
        "# we do have NaN values if we see the output above\n",
        "\n",
        "data.isnull().any().any()"
      ],
      "metadata": {
        "_uuid": "907fb8cf54c4dd53decd137bbc625c13d6a483cf",
        "trusted": true,
        "id": "254DKEKejHMt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing all the NaN values with 0 as the values correspond to the test results.\n",
        "# since, the values are not present that means the values are not available or calculated\n",
        "# Absence of a signal is assumed to be no signal in the dataset\n",
        "# so better we not take median or mean and replace them with zeros\n",
        "\n",
        "data = data.replace(np.NaN, 0)\n",
        "\n",
        "# again, checking if there is any NULL values left\n",
        "data.isnull().any().any()"
      ],
      "metadata": {
        "_uuid": "02c9f7ba9ca23a207975117fb3d65c0302c8e8d5",
        "trusted": true,
        "id": "ZDpjXNomjHMt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Visualization"
      ],
      "metadata": {
        "id": "NQOa8NubjHMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_vals = data['Pass/Fail'].unique()  # [0, 1, 2]\n",
        "targets = [data.loc[data['Pass/Fail'] == val] for val in unique_vals]"
      ],
      "metadata": {
        "trusted": true,
        "id": "vjO4EGLmjHMu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(20,20))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "for target in targets:\n",
        "    sns.distplot(target['1'], hist=True, rug=True)\n",
        "plt.title('First Sensor Measurements', fontsize = 20)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "for target in targets:\n",
        "    sns.distplot(target['2'], hist=True, rug=True)\n",
        "plt.title('Second Sensor Measurements', fontsize = 20)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "for target in targets:\n",
        "    sns.distplot(target['3'], hist=True, rug=True)\n",
        "plt.title('Third Sensor Measurements', fontsize = 20)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "for target in targets:\n",
        "    sns.distplot(target['4'], hist=True, rug=True)\n",
        "plt.title('Fourth Sensor Measurements', fontsize = 20)\n",
        "\n",
        "#sns.add_legend()\n",
        "#plt.legend()\n",
        "fig.legend(labels=['Pass','Fail'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "LHoZVv6ljHMu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# pie chart\n",
        "# We have highly imbalanced class with only 6.6% failures and 93.4% pass\n",
        "\n",
        "labels = ['Pass', 'Fail']\n",
        "size = data['Pass/Fail'].value_counts()\n",
        "colors = ['blue', 'green']\n",
        "explode = [0, 0.1]\n",
        "\n",
        "plt.style.use('seaborn-deep')\n",
        "plt.rcParams['figure.figsize'] = (8, 8)\n",
        "plt.pie(size, labels =labels, colors = colors, explode = explode, autopct = \"%.2f%%\", shadow = True)\n",
        "plt.axis('off')\n",
        "plt.title('Target: Pass or Fail', fontsize = 20)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "data['Pass/Fail'].value_counts().plot(kind=\"bar\");"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "dba3f399b4afb6ada70f9aca2b30d7b54b995b36",
        "trusted": true,
        "id": "CSZXXP4kjHMv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# heatmap to get correlation\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (18, 18)\n",
        "sns.heatmap(data.corr(), cmap = \"YlGnBu\")\n",
        "plt.title('Correlation heatmap for the Data', fontsize = 20)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "bb8f617162435191ff3b06002aa7967dab320667",
        "trusted": true,
        "id": "PM3bYu72jHMv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning/Preprocessing"
      ],
      "metadata": {
        "id": "UHt6VeL2jHMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove the highly collinear features from data\n",
        "def remove_collinear_features(x, threshold):\n",
        "    '''\n",
        "    Objective:\n",
        "        Remove collinear features in a dataframe with a correlation coefficient\n",
        "        greater than the threshold. Removing collinear features can help a model\n",
        "        to generalize and improves the interpretability of the model.\n",
        "\n",
        "    Inputs:\n",
        "        x: features dataframe\n",
        "        threshold: features with correlations greater than this value are removed\n",
        "\n",
        "    Output:\n",
        "        dataframe that contains only the non-highly-collinear features\n",
        "    '''\n",
        "\n",
        "    # Calculate the correlation matrix\n",
        "    corr_matrix = x.corr()\n",
        "    iters = range(len(corr_matrix.columns) - 1)\n",
        "    drop_cols = []\n",
        "\n",
        "    # Iterate through the correlation matrix and compare correlations\n",
        "    for i in iters:\n",
        "        for j in range(i+1):\n",
        "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
        "            col = item.columns\n",
        "            row = item.index\n",
        "            val = abs(item.values)\n",
        "\n",
        "            # If correlation exceeds the threshold\n",
        "            if val >= threshold:\n",
        "                # Print the correlated features and the correlation value\n",
        "                print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
        "                drop_cols.append(col.values[0])\n",
        "\n",
        "    # Drop one of each pair of correlated columns\n",
        "    drops = set(drop_cols)\n",
        "    x = x.drop(columns=drops)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZfieSiGyjHMw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove columns having more than 70% correlation\n",
        "#Both positive and negative correlations are considered here\n",
        "data = remove_collinear_features(data,0.70)"
      ],
      "metadata": {
        "trusted": true,
        "id": "xspPiWK3jHMw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# deleting the first column\n",
        "\n",
        "data = data.drop(columns = ['Time'], axis = 1)\n",
        "\n",
        "# checking the shape of the data after deleting a column\n",
        "data.shape"
      ],
      "metadata": {
        "_uuid": "566c012436034d57114ddcdee44761af2252027f",
        "trusted": true,
        "id": "zxpCBUqjjHMw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "CMOIvqvgjHMx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# separating the dependent and independent data\n",
        "\n",
        "x = data.iloc[:,:306]\n",
        "y = data[\"Pass/Fail\"]\n",
        "\n",
        "# getting the shapes of new data sets x and y\n",
        "print(\"shape of x:\", x.shape)\n",
        "print(\"shape of y:\", y.shape)"
      ],
      "metadata": {
        "_uuid": "e31d7c24b5aee355d9b3fe585e9a7b39a0b370b9",
        "trusted": true,
        "id": "mfJR-rEMjHMx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting them into train test and split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n",
        "\n",
        "# gettiing the shapes\n",
        "print(\"shape of x_train: \", x_train.shape)\n",
        "print(\"shape of x_test: \", x_test.shape)\n",
        "print(\"shape of y_train: \", y_train.shape)\n",
        "print(\"shape of y_test: \", y_test.shape)"
      ],
      "metadata": {
        "_uuid": "db5c557cd76c704581c69d50e232bc427c371e13",
        "trusted": true,
        "id": "se-5GMyjjHMy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# standardization\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# creating a standard scaler\n",
        "sc = StandardScaler()\n",
        "\n",
        "# fitting independent data to the model\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.transform(x_test)\n"
      ],
      "metadata": {
        "_uuid": "b380c366688d8fab5ee8d04393be945a62161a36",
        "trusted": true,
        "id": "JZW9p_IsjHMz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost Algorithm (Normal data)"
      ],
      "metadata": {
        "id": "x1LgCHQUjHMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = XGBClassifier(random_state=1)\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZyfBTRjKjHMz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.rcParams['figure.figsize'] = (5, 5)\n",
        "sns.set(style = 'dark', font_scale = 1.4)\n",
        "sns.heatmap(cm, annot = True, annot_kws = {\"size\": 15})"
      ],
      "metadata": {
        "trusted": true,
        "id": "AOsQVGbqjHM0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", model.score(x_test,y_test)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "w9VzYNqSjHM0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In the failure class we have only 1 observation classified correctly for XGBoost but still has 94.5% accuracy as we predicted correctly on the observations that passed"
      ],
      "metadata": {
        "id": "veawaZhzjHM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Classifier (Normal Data)"
      ],
      "metadata": {
        "id": "rItlZG4CjHM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(n_estimators=100, random_state=1,verbose=0 )\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "NCqRXqtzjHM2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.rcParams['figure.figsize'] = (5, 5)\n",
        "sns.set(style = 'dark', font_scale = 1.4)\n",
        "sns.heatmap(cm, annot = True, annot_kws = {\"size\": 15})"
      ],
      "metadata": {
        "trusted": true,
        "id": "xgPXJjIkjHM2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", model.score(x_test,y_test)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "qAByYa5pjHM3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In the failure class we have no observation classified correctly for Random Forest but still has 94.5% accuracy as we predicted correctly on the observations that passed"
      ],
      "metadata": {
        "id": "OpSTUcJijHM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression (Normal Data)"
      ],
      "metadata": {
        "id": "WgGXaG_WjHM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(random_state=1)\n",
        "lr.fit(x_train, y_train)\n",
        "y_pred = lr.predict(x_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "4IxAy2XCjHM3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.rcParams['figure.figsize'] = (5, 5)\n",
        "sns.set(style = 'dark', font_scale = 1.4)\n",
        "sns.heatmap(cm, annot = True, annot_kws = {\"size\": 15})"
      ],
      "metadata": {
        "trusted": true,
        "id": "5FMoUj9HjHM3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", lr.score(x_test,y_test)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "wMh75hR9jHND"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In the failure class we have 4 observation classified correctly for logistic regression and 88% accuracy as we predicted correctly on the observations that passed. So even though this model has lesser accuracy it is preferable over previous models as at least it is classifying more observations in the failure class correctly"
      ],
      "metadata": {
        "id": "_nix0FEqjHND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lasso (Normal Data)"
      ],
      "metadata": {
        "id": "MXHa-hhWjHNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lasso = Lasso(alpha=0.1,random_state=1)\n",
        "lasso.fit(x_train,y_train)\n",
        "#print (\"Lasso model:\", (lasso.coef_))\n",
        "\n",
        "y_pred = lasso.predict(x_test)\n",
        "\n",
        "#Convert the sign of the predicted values as the classifier\n",
        "y_pred2 = np.sign(y_pred)"
      ],
      "metadata": {
        "trusted": true,
        "id": "1_npbCVIjHNE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", lasso.score(x_test,y_test)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Exa1brZ9jHNF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred2)\n",
        "sns.heatmap(cm, annot = True, cmap = 'rainbow')"
      ],
      "metadata": {
        "trusted": true,
        "id": "POJBDq1cjHNI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UnderSampling of the Dataset"
      ],
      "metadata": {
        "id": "tNsaPbPKjHNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Under Sampling - Check how many failure observations are there\n",
        "# We have 104 such observations\n",
        "\n",
        "failed_tests = np.array(data[data['Pass/Fail'] == 1].index)\n",
        "no_failed_tests = len(failed_tests)\n",
        "\n",
        "print(no_failed_tests)"
      ],
      "metadata": {
        "_uuid": "dd5735c18a1754cf290814aba17db6492906a9ef",
        "trusted": true,
        "id": "MuD1ferWjHNI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many pass observations are there\n",
        "# We have 1,463 such observations\n",
        "\n",
        "normal_indices = data[data['Pass/Fail'] == -1]\n",
        "no_normal_indices = len(normal_indices)\n",
        "\n",
        "print(no_normal_indices)"
      ],
      "metadata": {
        "_uuid": "b7a109b65f5e053683b644813e38c01ba57a1bdb",
        "trusted": true,
        "id": "PlCKIU99jHNJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 104 random observations from the pass class as well\n",
        "\n",
        "random_normal_indices = np.random.choice(no_normal_indices, size = no_failed_tests, replace = True)\n",
        "random_normal_indices = np.array(random_normal_indices)\n",
        "\n",
        "print(len(random_normal_indices))"
      ],
      "metadata": {
        "_uuid": "a68a58a098f22944866bf1237eee101a97f2c6eb",
        "trusted": true,
        "id": "5O7fy-53jHNJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting a 50-50 representation from both pass and fail classes\n",
        "under_sample = np.concatenate([failed_tests, random_normal_indices])\n",
        "print(len(under_sample))"
      ],
      "metadata": {
        "_uuid": "0f09fcd71a6e5d4ffa81b20fc04d3c45d53ea4f9",
        "trusted": true,
        "id": "7uefRiaOjHNJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the undersample data\n",
        "\n",
        "undersample_data = data.iloc[under_sample, :]"
      ],
      "metadata": {
        "_uuid": "129ed5439d0718833ba6fcb6b0017201f8edc107",
        "trusted": true,
        "id": "63Grig4kjHNK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# splitting the undersample dataset into x and y sets\n",
        "\n",
        "x = undersample_data.iloc[:, undersample_data.columns != 'Pass/Fail']\n",
        "y = undersample_data.iloc[:, undersample_data.columns == 'Pass/Fail']\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "_uuid": "eb1a45336de9e046f13e42b3fbf3cdfcaf0823f7",
        "trusted": true,
        "id": "-OQbnPl2jHNK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train_us, x_test_us, y_train_us, y_test_us = train_test_split(x, y, test_size = 0.3, random_state = 1)\n",
        "\n",
        "print(x_train_us.shape)\n",
        "print(y_train_us.shape)\n",
        "print(x_test_us.shape)\n",
        "print(y_test_us.shape)"
      ],
      "metadata": {
        "_uuid": "84fbf8c77f97a189bcbb26a666a62932ebd5af47",
        "trusted": true,
        "id": "ie7Ft4vjjHNK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# standardization\n",
        "\n",
        "sc = StandardScaler()\n",
        "x_train_us = sc.fit_transform(x_train_us)\n",
        "x_test_us = sc.transform(x_test_us)"
      ],
      "metadata": {
        "_uuid": "ce7f8332311792cc5652ef68d04acadbf9be9c7a",
        "trusted": true,
        "id": "Ah5AHcPGjHNL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Xg-Boost Classifier (Undersampling)"
      ],
      "metadata": {
        "id": "9W8AutdfjHNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = XGBClassifier(random_state=1)\n",
        "\n",
        "model.fit(x_train_us, y_train_us)\n",
        "\n",
        "y_pred = model.predict(x_test_us)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "_uuid": "dff1d8489f8956ada9eaa1b839c8229621fe109c",
        "trusted": true,
        "id": "Wrowxa1OjHNM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix after UnderSampling with XgBoost"
      ],
      "metadata": {
        "id": "-iQ4qwljjHNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "cm = confusion_matrix(y_test_us, y_pred)\n",
        "\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (5, 5)\n",
        "sns.set(style = 'dark', font_scale = 1.4)\n",
        "sns.heatmap(cm, annot = True, annot_kws = {\"size\": 15})\n",
        "\n",
        "# It is able to predict 26 defected semiconductors among 35 Semi-Conductors"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "0ca53465a1aa7636a381ae00ea31e16bebb8c79c",
        "trusted": true,
        "id": "jByIukpujHNM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", model.score(x_test,y_test)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "d4FiQVNojHNN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Search - XG Boost (Undersampling)"
      ],
      "metadata": {
        "id": "MrMjyrRejHNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Grid Search CV to find the best model with the best parameters\n",
        "\n",
        "\n",
        "\n",
        "parameters = [{'max_depth' : [1, 2, 3, 4, 5, 6],\n",
        "              'cv' : [2,4,6,8,10],\n",
        "              'random_state' : [1]}]\n",
        "\n",
        "grid_search = GridSearchCV(estimator = model, param_grid = parameters, scoring = 'accuracy',  n_jobs = -1)\n",
        "\n",
        "grid_search = grid_search.fit(x_train_us, y_train_us)\n",
        "best_accuracy = grid_search.best_score_\n",
        "best_parameters = grid_search.best_params_"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "_uuid": "76d3f95082e18dcce61ddd6915ff4bca53e53e8b",
        "trusted": true,
        "id": "GPWALmrcjHNN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Accuracy: \", best_accuracy*100)\n",
        "print(\"Best Parameter: \", best_parameters)"
      ],
      "metadata": {
        "_uuid": "cb4549d720b0426f58492c64f30d8aebbb2eb7a0",
        "trusted": true,
        "id": "nADudRGVjHNO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "weights = (y == 0).sum()/(1.0*(y == -1).sum())\n",
        "model = XGBClassifier(max_depth = 1, scale_pos_weights = weights, n_jobs = 4,random_state=1,cv=2)\n",
        "\n",
        "model.fit(x_train_us, y_train_us)\n",
        "\n",
        "y_pred = model.predict(x_test_us)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "_uuid": "9984f8c582c0180b4da269607de352ca87925757",
        "trusted": true,
        "id": "Vzz3NA1zjHNO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", model.score(x_test,y_test)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "_-uRJvodjHNO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion matrix XG Boost - Grid Search (Undersample)"
      ],
      "metadata": {
        "id": "9CSPu1bjjHNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test_us, y_pred)\n",
        "\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (5, 5)\n",
        "sns.set(style = 'dark', font_scale = 1.4)\n",
        "sns.heatmap(cm, annot = True, annot_kws = {\"size\": 15})"
      ],
      "metadata": {
        "trusted": true,
        "id": "-ZWFUaQljHNP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Most Important Features of the Model**"
      ],
      "metadata": {
        "id": "LngfmxNsjHNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the feature importances\n",
        "\n",
        "colors = plt.cm.spring(np.linspace(0, 1, 9))\n",
        "xgb.plot_importance(model, height = 1, color = colors, grid = True, importance_type = 'cover', show_values = False)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (100, 100)\n",
        "plt.xlabel('The F-Score for each features')\n",
        "plt.ylabel('Importances')\n",
        "plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "c3856b309647565538efc1172151b56c82814592",
        "trusted": true,
        "id": "fIhEbzCOjHNP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Over-Sampling with SMOTE"
      ],
      "metadata": {
        "id": "pM7IB76zjHNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "x_resample, y_resample  = SMOTE(random_state=1).fit_sample(x, y.values.ravel())\n",
        "\n",
        "print(x_resample.shape)\n",
        "print(y_resample.shape)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "_uuid": "c77a2d8a1119359a71faa452d073530d37ff47c0",
        "trusted": true,
        "id": "a1fQqezbjHNP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "x_train_os, x_test_os, y_train_os, y_test_os = train_test_split(x, y, test_size = 0.3, random_state = 1)\n",
        "\n",
        "print(x_train_os.shape)\n",
        "print(y_train_os.shape)\n",
        "print(x_test_os.shape)\n",
        "print(y_test_os.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "371pEiUPjHNQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# standardization\n",
        "\n",
        "\n",
        "\n",
        "sc = StandardScaler()\n",
        "x_train_os = sc.fit_transform(x_train_os)\n",
        "x_test_os = sc.transform(x_test_os)"
      ],
      "metadata": {
        "trusted": true,
        "id": "e5GoByKujHNQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Xg-Boost Classifier - Grid Search (Oversampling)"
      ],
      "metadata": {
        "id": "640kLn2HjHNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "\n",
        "model = XGBClassifier(random_state=1)\n",
        "\n",
        "model.fit(x_train_os, y_train_os)\n",
        "\n",
        "y_pred = model.predict(x_test_os)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "oulkvnxOjHNR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Grid Search CV to find the best model with the best parameters\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# making a parameters list\n",
        "parameters = [{'max_depth' : [1, 2, 3, 4, 5, 6],\n",
        "              'cv' : [2,4,6,8,10],\n",
        "              'random_state' : [1]}]\n",
        "\n",
        "# making a grid search model\n",
        "grid_search = GridSearchCV(estimator = model, param_grid = parameters, scoring = 'accuracy', n_jobs = -1)\n",
        "grid_search = grid_search.fit(x_train_os, y_train_os)\n",
        "\n",
        "# getting the results\n",
        "best_accuracy = grid_search.best_score_\n",
        "best_parameters = grid_search.best_params_\n",
        "\n"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "g4VtzOw0jHNR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Accuracy: \", best_accuracy)\n",
        "print(\"Best Parameter: \", best_parameters)"
      ],
      "metadata": {
        "trusted": true,
        "id": "LLePAV8xjHNS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "weights = (y == 0).sum()/(1.0*(y == -1).sum())\n",
        "model = XGBClassifier(max_depth = 1, scale_pos_weights = weights, n_jobs = 4,random_state=1,cv=2)\n",
        "\n",
        "model.fit(x_train_os, y_train_os)\n",
        "\n",
        "y_pred = model.predict(x_test_os)\n"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "KLK7lpefjHNT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix after OverSampling with XgBoost"
      ],
      "metadata": {
        "id": "bCY0ovZ_jHNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "cm = confusion_matrix(y_test_os, y_pred)\n",
        "\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (5, 5)\n",
        "sns.set(style = 'dark', font_scale = 1.4)\n",
        "sns.heatmap(cm, annot = True, annot_kws = {\"size\": 15}, cmap = 'spring')"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "N4pHXWNLjHNW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", model.score(x_test,y_test)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "nkg-_4ZKjHNX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest (Oversampling)"
      ],
      "metadata": {
        "id": "mAalyrbqjHNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(n_estimators=100, random_state=1,verbose=0 )\n",
        "model.fit(x_train_os, y_train_os)\n",
        "#scores_prediction = model.decision_function(x_train)\n",
        "y_pred = model.predict(x_test_os)\n",
        "print(\"Accuracy: \", model.score(x_test_os,y_test_os)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "t-cA0sqcjHNY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest - (Oversampled) - Confusion Matrix"
      ],
      "metadata": {
        "id": "3RjqircejHNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the confusion matrix\n",
        "cm = confusion_matrix(y_test_os, y_pred)\n",
        "sns.heatmap(cm, annot = True, cmap = 'rainbow')"
      ],
      "metadata": {
        "trusted": true,
        "id": "edsBJn4kjHNY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic regression - (Oversampled)"
      ],
      "metadata": {
        "id": "Ur7wQJ1-jHNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(random_state=1)\n",
        "lr.fit(x_train_os, y_train_os)\n",
        "y_pred = lr.predict(x_test_os)\n",
        "\n",
        "print(\"Accuracy: \", lr.score(x_test_os,y_test_os)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "cvy_EHapjHNZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic regression (Oversampled) - Confusion matrix"
      ],
      "metadata": {
        "id": "AXMKqktAjHNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test_os, y_pred)\n",
        "sns.heatmap(cm, annot = True, cmap = 'rainbow')"
      ],
      "metadata": {
        "trusted": true,
        "id": "OJrrgmFgjHNa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest - (Undersampled)"
      ],
      "metadata": {
        "id": "QfuaBWaDjHNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=1,verbose=0 )\n",
        "model.fit(x_train_us, y_train_us)\n",
        "#scores_prediction = model.decision_function(x_train)\n",
        "y_pred = model.predict(x_test_us)"
      ],
      "metadata": {
        "trusted": true,
        "id": "x6KdP6BsjHNb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix for Random Forest (Undersampled)"
      ],
      "metadata": {
        "id": "a2mpHjIDjHNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating the model\n",
        "\n",
        "# printing the confusion matrix\n",
        "cm = confusion_matrix(y_test_us, y_pred)\n",
        "sns.heatmap(cm, annot = True, cmap = 'rainbow')"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "qdWn6RYZjHNc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", model.score(x_test,y_test)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "NeQ7jcxgjHNc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Lasso (Undersampled)"
      ],
      "metadata": {
        "id": "Vk3dmA7DjHNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lasso = Lasso(alpha=0.1,random_state=1)\n",
        "lasso.fit(x_train_us,y_train_us)\n",
        "#print (\"Lasso model:\", (lasso.coef_))"
      ],
      "metadata": {
        "trusted": true,
        "id": "1subp5x2jHNc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lasso.predict(x_test_us)"
      ],
      "metadata": {
        "trusted": true,
        "id": "60KMAMEdjHNc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pTiqz6IcjHNd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test_us)"
      ],
      "metadata": {
        "trusted": true,
        "id": "2VtbMm3vjHNd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert the sign of the predicted values as the classifier\n",
        "y_pred2 = np.sign(y_pred)"
      ],
      "metadata": {
        "trusted": true,
        "id": "71wSpPfQjHNd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion matrix for Lasso (Undersampled)"
      ],
      "metadata": {
        "id": "e0zvXtNojHNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test_us, y_pred2)\n",
        "sns.heatmap(cm, annot = True, cmap = 'rainbow')"
      ],
      "metadata": {
        "trusted": true,
        "id": "phHEkSuIjHNd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", lasso.score(x_test_us,y_test_us)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "N54Fpbc9jHNd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression (Undersampled)"
      ],
      "metadata": {
        "id": "BpC0T6qEjHNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(random_state=1)\n",
        "lr.fit(x_train_us, y_train_us)"
      ],
      "metadata": {
        "trusted": true,
        "id": "qpiAHPEVjHNe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lr.predict(x_test_us)"
      ],
      "metadata": {
        "trusted": true,
        "id": "WgQ8AB1CjHNe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion matrix for Logistic Regression (Undersampled)"
      ],
      "metadata": {
        "id": "8dfKEeUmjHNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test_us, y_pred)\n",
        "sns.heatmap(cm, annot = True, cmap = 'rainbow')"
      ],
      "metadata": {
        "trusted": true,
        "id": "gRYW_NgbjHNe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", lr.score(x_test,y_test)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "0PjgySBOjHNe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Local Outlier Factor Classifier,** Succesfully detected 11 defected items out of 13, that makes an impeccable accuracy of 85% Recall Accuracy. This algorithm would have been worked even better if the data was a little bigger with more instances of defected items."
      ],
      "metadata": {
        "id": "hR20WyKljHNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ods9vnY6jHNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Class SVM (Undersampled)"
      ],
      "metadata": {
        "id": "RvcQVyYZjHNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> In one-class SVM, the support vector model is trained on data that has only one class, which is the “normal” class. It infers the properties of normal cases and from these properties can predict which examples are unlike the normal examples. This is useful for anomaly detection because the scarcity of training examples is what defines anomalies: that is, typically there are very few examples of the network intrusion, fraud, or other anomalous behavior."
      ],
      "metadata": {
        "id": "vo3SK7qCjHNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = OneClassSVM(kernel ='rbf', degree=3, gamma=0.1,nu=0.005, max_iter=-1, random_state=1)\n",
        "\n",
        "model.fit(x_train_us, y_train_us)\n",
        "y_pred = model.fit_predict(x_test_us)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "0gdxVeRmjHNf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix for One Class SVM"
      ],
      "metadata": {
        "id": "YmnVRDl1jHNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# evaluating the model\n",
        "# printing the confusion matrix\n",
        "cm = confusion_matrix(y_test_us, y_pred)\n",
        "sns.heatmap(cm ,annot = True, cmap = 'winter')\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ARS_jgWyjHNf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"Accuracy: \", model.score(x_test,y_test)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "KlUE5BN8jHNg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OneClassSVM - (Oversampled)"
      ],
      "metadata": {
        "id": "zE_uA1eWjHNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = OneClassSVM(kernel ='rbf', degree=3, gamma=0.1,nu=0.005, max_iter=-1, random_state=1)\n",
        "\n",
        "model.fit(x_train_os, y_train_os)\n",
        "y_pred = model.fit_predict(x_test_os)"
      ],
      "metadata": {
        "trusted": true,
        "id": "jpKkqL3-jHNg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion matrix for OneClassSVM - (Oversampled)"
      ],
      "metadata": {
        "id": "AzFHRDF2jHNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating the model\n",
        "# printing the confusion matrix\n",
        "cm = confusion_matrix(y_test_os, y_pred)\n",
        "sns.heatmap(cm ,annot = True, cmap = 'winter')"
      ],
      "metadata": {
        "trusted": true,
        "id": "QsdoKgVCjHNg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using PCA for demensionality reduction"
      ],
      "metadata": {
        "id": "hhCNwY3sjHNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaling the data before applying PCA\n",
        "from scipy.stats import zscore\n",
        "data_new=data.iloc[:,:306].apply(zscore)\n",
        "data_new.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "wAXtY9JKjHNg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data_new.isnull().any().any()"
      ],
      "metadata": {
        "trusted": true,
        "id": "8uXBLgaCjHNh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data_new = data_new.replace(np.NaN, 0)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Zi4SViXmjHNh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data_new.isnull().any().any()"
      ],
      "metadata": {
        "trusted": true,
        "id": "qHDDQT3cjHNh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# separating the dependent and independent data\n",
        "\n",
        "x = data_new.iloc[:,:306]\n",
        "y = data[\"Pass/Fail\"]\n",
        "\n",
        "# getting the shapes of new data sets x and y\n",
        "print(\"shape of x:\", x.shape)\n",
        "print(\"shape of y:\", y.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "uSytDSJYjHNh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "QCb3TqnOjHNl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA\n",
        "# Step 1 - Create covariance matrix\n",
        "\n",
        "cov_matrix = np.cov(x.T)\n",
        "print('Covariance Matrix \\n%s', cov_matrix)"
      ],
      "metadata": {
        "trusted": true,
        "id": "fm7dv6DOjHNl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2- Get eigen values and eigen vector\n",
        "eig_vals, eig_vecs = np.linalg.eig(cov_matrix)\n",
        "print('Eigen Vectors \\n%s', eig_vecs)\n",
        "print('\\n Eigen Values \\n%s', eig_vals)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ugBsBmdgjHNm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tot = sum(eig_vals)\n",
        "var_exp = [( i /tot ) * 100 for i in sorted(eig_vals, reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "print(\"Cumulative Variance Explained\", cum_var_exp)"
      ],
      "metadata": {
        "trusted": true,
        "id": "0nOy40ZojHNm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(var_exp)"
      ],
      "metadata": {
        "trusted": true,
        "id": "htzasMamjHNn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Ploting\n",
        "plt.figure(figsize=(10 , 5))\n",
        "plt.bar(range(1, eig_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\n",
        "plt.step(range(1, eig_vals.size + 1), cum_var_exp, where='mid', label = 'Cumulative explained variance')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.xlabel('Principal Components')\n",
        "plt.legend(loc = 'best')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "r7a5xwkmjHNn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "len(cum_var_exp)"
      ],
      "metadata": {
        "trusted": true,
        "id": "L_RPJSZejHNn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Using scikit learn PCA here. It does all the above steps and maps data to PCA dimensions in one shot\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# NOTE - we are generating only 130 PCA dimensions (dimensionality reduction from 306 to 130)\n",
        "# For 130 components we are getting approximately 90% of the variance\n",
        "pca = PCA(n_components=130)\n",
        "data_reduced = pca.fit_transform(x)\n",
        "data_reduced.transpose()"
      ],
      "metadata": {
        "trusted": true,
        "id": "cwVEeuZtjHNn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pca.components_"
      ],
      "metadata": {
        "trusted": true,
        "id": "Z_t5SkdkjHNn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_comp = pd.DataFrame(pca.components_,columns=list(x))\n",
        "df_comp.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "6KjFO8JEjHNo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(df_comp,cmap='plasma',)"
      ],
      "metadata": {
        "trusted": true,
        "id": "lTz6TxxajHNo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data_reduced.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "pDqBRdrAjHNo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_red2 = pd.DataFrame(data_reduced)\n",
        "df_red2.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Zfn2io5ujHNo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_red3 = df_red2.copy()\n",
        "df_red4 = df_red3\n",
        "df_red4[\"Pass/Fail\"] = data[\"Pass/Fail\"]"
      ],
      "metadata": {
        "trusted": true,
        "id": "nUsOzkx5jHNo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_red4.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "vmDfJnyAjHNp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_red4.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "Viaz9As-jHNp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Causing system crash and taking too much time, so commented out\n",
        "#sns.pairplot(df_red4,diag_kind='kde')"
      ],
      "metadata": {
        "trusted": true,
        "id": "rrsbz6WCjHNp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample boxplot shows that there are outliers in the data, let us fix them\n",
        "df_red4.boxplot(column = [df_red4.columns[0],\n",
        "                          df_red4.columns[1],\n",
        "                          df_red4.columns[2],\n",
        "                          df_red4.columns[3],\n",
        "                          df_red4.columns[4],\n",
        "                          df_red4.columns[5],\n",
        "                         ]\n",
        "                          , by = 'Pass/Fail', figsize=(20,20))"
      ],
      "metadata": {
        "trusted": true,
        "id": "PWw4XI-KjHNq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a copy of the dataset for maintain data after outlier removal\n",
        "#Here after identifying outliers we replace with median\n",
        "pd_data = df_red4.copy()\n",
        "#pd_data.head()\n",
        "\n",
        "#pd_data2 = pd_data.drop(columns=['name'],axis=1)\n",
        "#pd_data2 = pd_data2.apply(replace,axis=1)\n",
        "from scipy import stats\n",
        "\n",
        "#Define a function to remove outliers on max side\n",
        "def outlier_removal_max(var):\n",
        "    var = np.where(var > var.quantile(0.75)+ stats.iqr(var),var.quantile(0.50),var)\n",
        "    return var\n",
        "\n",
        "#Define a function to remove outliers on min side\n",
        "def outlier_removal_min(var):\n",
        "    var = np.where(var < var.quantile(0.25) - stats.iqr(var),var.quantile(0.50),var)\n",
        "    return var\n",
        "\n",
        "#Loop over the columns and remove the outliers on min and max side\n",
        "for column in pd_data:\n",
        "    pd_data[column] = outlier_removal_max(pd_data[column])\n",
        "    pd_data[column] = outlier_removal_min(pd_data[column])"
      ],
      "metadata": {
        "trusted": true,
        "id": "iaUQSWCUjHNq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample boxplot shows that outliers are fixed, but we are loosing observations belonging to failure\n",
        "#class (Pass/Fail = 1) So we should not remove outliers here\n",
        "pd_data.boxplot( column =[df_red4.columns[0],\n",
        "                          df_red4.columns[1],\n",
        "                          df_red4.columns[2],\n",
        "                          df_red4.columns[3],\n",
        "                          df_red4.columns[4],\n",
        "                          df_red4.columns[5],\n",
        "                         ],by = 'Pass/Fail', figsize=(20,20))"
      ],
      "metadata": {
        "trusted": true,
        "id": "OmNilWmCjHNr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# separating the dependent and independent data\n",
        "\n",
        "x = df_red4.iloc[:, df_red4.columns != 'Pass/Fail']\n",
        "y = df_red4.iloc[:, df_red4.columns == 'Pass/Fail']\n",
        "\n",
        "# getting the shapes of new data sets x and y\n",
        "print(\"shape of x:\", x.shape)\n",
        "print(\"shape of y:\", y.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "XV3h603jjHNr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Under Sampling - Check how many failure observations are there\n",
        "# We have 104 such observations\n",
        "\n",
        "failed_tests = np.array(df_red4[df_red4['Pass/Fail'] == 1].index)\n",
        "no_failed_tests = len(failed_tests)\n",
        "\n",
        "print(no_failed_tests)"
      ],
      "metadata": {
        "trusted": true,
        "id": "uzyfTca0jHNr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many pass observations are there\n",
        "# We have 1,463 such observations\n",
        "\n",
        "normal_indices = df_red4[df_red4['Pass/Fail'] == -1]\n",
        "no_normal_indices = len(normal_indices)\n",
        "\n",
        "print(no_normal_indices)"
      ],
      "metadata": {
        "trusted": true,
        "id": "DP1_cXY0jHNs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 104 random observations from the pass class as well\n",
        "\n",
        "random_normal_indices = np.random.choice(no_normal_indices, size = no_failed_tests, replace = True)\n",
        "random_normal_indices = np.array(random_normal_indices)\n",
        "\n",
        "print(len(random_normal_indices))"
      ],
      "metadata": {
        "trusted": true,
        "id": "Wxl1pZWkjHNs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting a 50-50 representation from both pass and fail classes\n",
        "under_sample = np.concatenate([failed_tests, random_normal_indices])\n",
        "print(len(under_sample))"
      ],
      "metadata": {
        "trusted": true,
        "id": "04iemXUzjHNt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the undersample data\n",
        "\n",
        "undersample_data = df_red4.iloc[under_sample, :]\n",
        "\n",
        "# splitting the undersample dataset into x and y sets\n",
        "\n",
        "x = undersample_data.iloc[:, undersample_data.columns != 'Pass/Fail']\n",
        "y = undersample_data.iloc[:, undersample_data.columns == 'Pass/Fail']\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "rDl2cJd1jHNt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train_us, x_test_us, y_train_us, y_test_us = train_test_split(x, y, test_size = 0.3, random_state = 1)\n",
        "\n",
        "print(x_train_us.shape)\n",
        "print(y_train_us.shape)\n",
        "print(x_test_us.shape)\n",
        "print(y_test_us.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "82iAv7GhjHNt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# standardization - completed before PCA itself\n",
        "\n",
        "#sc = StandardScaler()\n",
        "#x_train_us = sc.fit_transform(x_train_us)\n",
        "#x_test_us = sc.transform(x_test_us)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "suGsksI_jHNt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost - PCA (undersampled)"
      ],
      "metadata": {
        "id": "oczk-AFmjHNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = XGBClassifier(random_state=1)\n",
        "\n",
        "model.fit(x_train_us, y_train_us)\n",
        "\n",
        "y_pred = model.predict(x_test_us)\n",
        "\n",
        "cm = confusion_matrix(y_test_us, y_pred)"
      ],
      "metadata": {
        "trusted": true,
        "id": "if3qbFDAjHNt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion matrix for XG Boost - PCA - (Undersampled)"
      ],
      "metadata": {
        "id": "i5tEkQcPjHNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = (5, 5)\n",
        "sns.set(style = 'dark', font_scale = 1.4)\n",
        "sns.heatmap(cm, annot = True, annot_kws = {\"size\": 15})\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "RYtbJsMnjHNu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", model.score(x_test_us,y_test_us)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "eYml2iqXjHNu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XG Boost - PCA - Grid Search - Undersampled"
      ],
      "metadata": {
        "id": "3KP2aE09jHNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Grid Search CV to find the best model with the best parameters\n",
        "\n",
        "\n",
        "\n",
        "parameters = [{'max_depth' : [1, 2, 3, 4, 5, 6],\n",
        "              'cv' : [2,4,6,8,10],\n",
        "             'random_state' : [1]}]\n",
        "\n",
        "grid_search = GridSearchCV(estimator = model, param_grid = parameters, scoring = 'accuracy', n_jobs = -1)\n",
        "\n",
        "grid_search = grid_search.fit(x_train_us, y_train_us)\n",
        "best_accuracy = grid_search.best_score_\n",
        "best_parameters = grid_search.best_params_\n",
        "\n",
        "print(\"Best Accuracy: \", best_accuracy*100)\n",
        "print(\"Best Parameter: \", best_parameters)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Lcc4ed1bjHNu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "RBKvHhcZjHNu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "weights = (y == 0).sum()/(1.0*(y == -1).sum())\n",
        "model = XGBClassifier(max_depth = 1, scale_pos_weights = weights, n_jobs = 4,random_state=1,cv=2)\n",
        "\n",
        "model.fit(x_train_us, y_train_us)\n",
        "\n",
        "y_pred = model.predict(x_test_us)\n",
        "\n",
        "print(\"Accuracy: \", model.score(x_test_us,y_test_us)*100)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "iIrh2BFxjHNu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xmw2xHS5jHN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion matrix of XG Boost - Grid Search - PCA - Undersampled"
      ],
      "metadata": {
        "id": "bSg96bU-jHN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test_us, y_pred)\n",
        "\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (5, 5)\n",
        "sns.set(style = 'dark', font_scale = 1.4)\n",
        "sns.heatmap(cm, annot = True, annot_kws = {\"size\": 15})\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "m67QjbDtjHN6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest - PCA - Undersampled"
      ],
      "metadata": {
        "id": "9kO57Pi2jHN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(n_estimators=100, random_state=1,verbose=0 )\n",
        "model.fit(x_train_us, y_train_us)\n",
        "#scores_prediction = model.decision_function(x_train)\n",
        "y_pred = model.predict(x_test_us)\n",
        "\n",
        "# evaluating the model\n",
        "\n",
        "# printing the confusion matrix\n",
        "cm = confusion_matrix(y_test_us, y_pred)\n",
        "sns.heatmap(cm, annot = True, cmap = 'rainbow')\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "wANWCR3jjHN7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", model.score(x_test_us,y_test_us)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Yw8y8HU2jHN8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression - PCA - Undersampled"
      ],
      "metadata": {
        "id": "QlZwNOXujHN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(random_state=1)\n",
        "lr.fit(x_train_us, y_train_us)\n",
        "y_pred = lr.predict(x_test_us)\n",
        "cm = confusion_matrix(y_test_us, y_pred)\n",
        "sns.heatmap(cm, annot = True, cmap = 'rainbow')\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ctB2daZxjHN9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", lr.score(x_test_us,y_test_us)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "S0vFRKjBjHN9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lasso - PCA - Undersampled"
      ],
      "metadata": {
        "id": "qp4tyWBWjHN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lasso = Lasso(alpha=0.1,random_state=1)\n",
        "lasso.fit(x_train_us,y_train_us)\n",
        "#print (\"Lasso model:\", (lasso.coef_))\n",
        "\n",
        "y_pred = lasso.predict(x_test_us)\n",
        "\n",
        "#Convert the sign of the predicted values as the classifier\n",
        "y_pred2 = np.sign(y_pred)"
      ],
      "metadata": {
        "trusted": true,
        "id": "59itfIdnjHN_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "actual_cost = list(y_test_us)\n",
        "actual_cost = np.asarray(actual_cost)\n",
        "y_pred_lass = lasso.predict(x_test_us)"
      ],
      "metadata": {
        "trusted": true,
        "id": "gmwuUk7SjHN_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", lasso.score(x_test_us, y_test_us)*100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Hz8OrbPHjHN_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix for Lasso - PCA - Undersampled"
      ],
      "metadata": {
        "id": "nf3sbazhjHOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cm = confusion_matrix(y_test_us, y_pred2)\n",
        "sns.heatmap(cm, annot = True, cmap = 'rainbow')"
      ],
      "metadata": {
        "trusted": true,
        "id": "HcufTjHFjHOA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elliptic Envelop technique"
      ],
      "metadata": {
        "id": "shix-6AijHOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining outlier fraction for Elliptic Envelop, Isolation Forest and Local Outlier Factor\n",
        "\n",
        "Fraud = data[data['Pass/Fail']==1]\n",
        "Valid = data[data['Pass/Fail']==-1]\n",
        "\n",
        "outlier_fraction = len(Fraud)/float(len(Valid))\n",
        "print(\"Outlier Fraction :\", outlier_fraction)"
      ],
      "metadata": {
        "trusted": true,
        "id": "xeH1N59xjHOB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.covariance import EllipticEnvelope\n",
        "model = EllipticEnvelope(contamination=outlier_fraction, random_state=1)\n",
        "model.fit(x_train, y_train)\n",
        "#scores_prediction = model.decision_function(x_train)\n",
        "y_pred = model.predict(x_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "lKWBEc0djHOB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot = True, cmap = 'rainbow')"
      ],
      "metadata": {
        "trusted": true,
        "id": "M4DfQawujHOB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Isolation Forest technique"
      ],
      "metadata": {
        "id": "rGGUYxWljHOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "model = IsolationForest(n_estimators=100, max_samples=len(x_train),\n",
        "                                       contamination=outlier_fraction, random_state=1, verbose=0)\n",
        "model.fit(x_train, y_train)\n",
        "scores_prediction = model.decision_function(x_train)\n",
        "y_pred = model.predict(x_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "G4EmwwDsjHOB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot = True, cmap = 'rainbow')"
      ],
      "metadata": {
        "trusted": true,
        "id": "U1o9oO-0jHOC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Outlier Factor Technique"
      ],
      "metadata": {
        "id": "S7ALuktvjHOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "\n",
        "model = LocalOutlierFactor(n_neighbors=20, algorithm='auto', leaf_size=30, metric='minkowski', p=2,\n",
        "                           metric_params=None, contamination=outlier_fraction)\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.fit_predict(x_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pfGhL27AjHOC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot = True, cmap = 'rainbow')"
      ],
      "metadata": {
        "trusted": true,
        "id": "nAuJby_0jHOC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison of All Algorithms' Recall Rate"
      ],
      "metadata": {
        "id": "rCTaRl3wjHOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Recall = np.array([84.4, 87.9, 81.3,9.4,0,0,0])\n",
        "label = np.array(['Isolation Forest', 'Local Outlier Factor', 'Elliptic Envelop',\n",
        "                  'Logistic','XGBoost','Random Forest','Lasso'])\n",
        "indices = np.argsort(Recall)\n",
        "color = plt.cm.rainbow(np.linspace(0, 1, 9))\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (18, 7)\n",
        "plt.bar(range(len(indices)), Recall[indices], color = color)\n",
        "plt.xticks(range(len(indices)), label[indices])\n",
        "plt.title('Recall Accuracy - Normal Data', fontsize = 30)\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "BapgMO2CjHOC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Recall = np.array([74.3,77.1,71.4,77.1,71.4,17.1])\n",
        "label = np.array(['XG Boost','XG Boost - Grid Search','Random Forest','Logistic','Lasso','OneClass SVM'])\n",
        "indices = np.argsort(Recall)\n",
        "color = plt.cm.rainbow(np.linspace(0, 1, 9))\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (18, 7)\n",
        "plt.bar(range(len(indices)), Recall[indices], color = color)\n",
        "plt.xticks(range(len(indices)), label[indices])\n",
        "plt.title('Recall Accuracy - Undersampled Data', fontsize = 30)\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "yvANf_FbjHOD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Recall = np.array([74.3,77.1,71.4,77.1,71.4,17.1])\n",
        "label = np.array(['XG Boost','XG Boost - Grid Search','Random Forest','Logistic','Lasso','OneClass SVM'])\n",
        "indices = np.argsort(Recall)\n",
        "color = plt.cm.rainbow(np.linspace(0, 1, 9))\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (18, 7)\n",
        "plt.bar(range(len(indices)), Recall[indices], color = color)\n",
        "plt.xticks(range(len(indices)), label[indices])\n",
        "plt.title('Recall Accuracy - Oversampled Data', fontsize = 30)\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "41sx9IFEjHOD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Recall = np.array([54.5,63.6,48.5,69.7,51.5])\n",
        "label = np.array(['XG Boost','XG Boost - Grid Search','Random Forest','Logistic','Lasso'])\n",
        "indices = np.argsort(Recall)\n",
        "color = plt.cm.rainbow(np.linspace(0, 1, 9))\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (18, 7)\n",
        "plt.bar(range(len(indices)), Recall[indices], color = color)\n",
        "plt.xticks(range(len(indices)), label[indices])\n",
        "plt.title('Recall Accuracy - PCA Data', fontsize = 30)\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "HBNv8qrdjHOE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the feature importances\n",
        "\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "pyplot.rcParams.update({'font.size': 22})\n",
        "from matplotlib.pyplot import figure\n",
        "figure(num=None, figsize=(100, 100), dpi=80, facecolor='w', edgecolor='k')\n",
        "# fit model no training data\n",
        "model = XGBClassifier()\n",
        "model.fit(x_train_us, y_train_us)\n",
        "# plot feature importance\n",
        "plot_importance(model)\n",
        "pyplot.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "H89RmK1jjHOF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "featureImp = []\n",
        "for feat, importance in zip(data.columns, model.feature_importances_):\n",
        "    temp = [feat, importance*100]\n",
        "    featureImp.append(temp)\n",
        "\n",
        "fT_df = pd.DataFrame(featureImp, columns = ['Feature', 'Importance'])\n",
        "print (fT_df.sort_values('Importance', ascending = False))"
      ],
      "metadata": {
        "trusted": true,
        "id": "sS7P8KPbjHOF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We have tried multiple models Logistic Regression, Random Forest, XG Boost (with and without Grid Search),OneClassSVM, Elliptic Envelop, Isolation Forest and Local Outlier Factor for the imbalanced classes\n",
        "## Across methods OneClassSVM performed the worst while Local Outlier Factor performed the best in terms of recall accuracy\n",
        "## We saw that for imbalanced classes accuracy and recall are invertially proportional to each other. Better recall models have lower accuracy and vice versa.\n",
        "## We have tried two sampling techniques -first one using SMOTE (oversampling) and second one\n",
        "## using random based method (undersampling), Oversampling gave better results than undersampling in\n",
        "## terms of accuracy. Recall score was similar for both undersampling and oversampling.\n",
        "## We did Z score scaling on both the datasets and took PCA with n_components as 130 (90% variance coverage). However PCA did not improve either accuracy or recall probably as we were loosing information due to dropping dimensions.\n",
        "## We tried K-fold cross validation within XG Boost itself with bext value as 2 for all the models\n",
        "## Using feature importance, we found that 72, 74 and 45 are the top three important\n",
        "## features. The best recall value at 88% was for Local Outlier Factor algorithm, the best part was no sampling was required as the algorithm took care of sampling as well as outliers.\n",
        "## classifier on the failure observations"
      ],
      "metadata": {
        "id": "pprjrN9TjHOG"
      }
    }
  ]
}